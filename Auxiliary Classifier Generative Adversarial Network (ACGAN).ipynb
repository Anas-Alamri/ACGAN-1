{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "from PIL import Image\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2DTranspose, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.generic_utils import Progbar\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_size):\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Dense(3 * 3 * 384, input_dim=latent_size, activation='relu'))\n",
    "    cnn.add(Reshape((3, 3, 384)))\n",
    "\n",
    "    cnn.add(Conv2DTranspose(192, 5, strides=1, padding='valid',\n",
    "                            activation='relu',\n",
    "                            kernel_initializer='glorot_normal'))\n",
    "    cnn.add(BatchNormalization())\n",
    "\n",
    "    cnn.add(Conv2DTranspose(96, 5, strides=2, padding='same',\n",
    "                            activation='relu',\n",
    "                            kernel_initializer='glorot_normal'))\n",
    "    cnn.add(BatchNormalization())\n",
    "\n",
    "    cnn.add(Conv2DTranspose(1, 5, strides=2, padding='same',\n",
    "                            activation='tanh',\n",
    "                            kernel_initializer='glorot_normal'))\n",
    "\n",
    "    latent = Input(shape=(latent_size, ))\n",
    "\n",
    "    image_class = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    cls = Embedding(num_classes, latent_size,\n",
    "                    embeddings_initializer='glorot_normal')(image_class)\n",
    "\n",
    "    h = layers.multiply([latent, cls])\n",
    "\n",
    "    fake_image = cnn(h)\n",
    "\n",
    "    return Model([latent, image_class], fake_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Conv2D(32, 3, padding='same', strides=2,\n",
    "                   input_shape=(28, 28, 1)))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Conv2D(64, 3, padding='same', strides=1))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Conv2D(128, 3, padding='same', strides=2))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Conv2D(256, 3, padding='same', strides=1))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "\n",
    "    image = Input(shape=(28, 28, 1))\n",
    "\n",
    "    features = cnn(image)\n",
    "\n",
    "    fake = Dense(1, activation='sigmoid', name='generation')(features)\n",
    "    aux = Dense(num_classes, activation='softmax', name='auxiliary')(features)\n",
    "\n",
    "    return Model(image, [fake, aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    latent_size = 100\n",
    "\n",
    "    adam_lr = 0.0002\n",
    "    adam_beta_1 = 0.5\n",
    "\n",
    "    # build the discriminator\n",
    "    print('Discriminator model:')\n",
    "    discriminator = build_discriminator()\n",
    "    discriminator.compile(\n",
    "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "    )\n",
    "    discriminator.summary()\n",
    "\n",
    "    generator = build_generator(latent_size)\n",
    "\n",
    "    latent = Input(shape=(latent_size, ))\n",
    "    image_class = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    fake = generator([latent, image_class])\n",
    "\n",
    "    discriminator.trainable = False\n",
    "    fake, aux = discriminator(fake)\n",
    "    combined = Model([latent, image_class], [fake, aux])\n",
    "\n",
    "    print('Combined model:')\n",
    "    combined.compile(\n",
    "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "    )\n",
    "    combined.summary()\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "    x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "    num_train, num_test = x_train.shape[0], x_test.shape[0]\n",
    "\n",
    "    train_history = defaultdict(list)\n",
    "    test_history = defaultdict(list)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs))\n",
    "\n",
    "        num_batches = int(np.ceil(x_train.shape[0] / float(batch_size)))\n",
    "        progress_bar = Progbar(target=num_batches)\n",
    "\n",
    "        epoch_gen_loss = []\n",
    "        epoch_disc_loss = []\n",
    "\n",
    "        for index in range(num_batches):\n",
    "            image_batch = x_train[index * batch_size:(index + 1) * batch_size]\n",
    "            label_batch = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "            noise = np.random.uniform(-1, 1, (len(image_batch), latent_size))\n",
    "\n",
    "            sampled_labels = np.random.randint(0, num_classes, len(image_batch))\n",
    "\n",
    "\n",
    "            generated_images = generator.predict(\n",
    "                [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n",
    "\n",
    "            x = np.concatenate((image_batch, generated_images))\n",
    "\n",
    "            soft_zero, soft_one = 0, 0.95\n",
    "            y = np.array(\n",
    "                [soft_one] * len(image_batch) + [soft_zero] * len(image_batch))\n",
    "            aux_y = np.concatenate((label_batch, sampled_labels), axis=0)\n",
    "\n",
    "            disc_sample_weight = [np.ones(2 * len(image_batch)),\n",
    "                                  np.concatenate((np.ones(len(image_batch)) * 2,\n",
    "                                                  np.zeros(len(image_batch))))]\n",
    "\n",
    "            epoch_disc_loss.append(discriminator.train_on_batch(\n",
    "                x, [y, aux_y], sample_weight=disc_sample_weight))\n",
    "\n",
    "            noise = np.random.uniform(-1, 1, (2 * len(image_batch), latent_size))\n",
    "            sampled_labels = np.random.randint(0, num_classes, 2 * len(image_batch))\n",
    "\n",
    "            trick = np.ones(2 * len(image_batch)) * soft_one\n",
    "\n",
    "            epoch_gen_loss.append(combined.train_on_batch(\n",
    "                [noise, sampled_labels.reshape((-1, 1))],\n",
    "                [trick, sampled_labels]))\n",
    "\n",
    "            progress_bar.update(index + 1)\n",
    "\n",
    "        print('Testing for epoch {}:'.format(epoch))\n",
    "\n",
    "        noise = np.random.uniform(-1, 1, (num_test, latent_size))\n",
    "\n",
    "        sampled_labels = np.random.randint(0, num_classes, num_test)\n",
    "        generated_images = generator.predict(\n",
    "            [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n",
    "\n",
    "        x = np.concatenate((x_test, generated_images))\n",
    "        y = np.array([1] * num_test + [0] * num_test)\n",
    "        aux_y = np.concatenate((y_test, sampled_labels), axis=0)\n",
    "\n",
    "        discriminator_test_loss = discriminator.evaluate(\n",
    "            x, [y, aux_y], verbose=False)\n",
    "\n",
    "        discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n",
    "\n",
    "        noise = np.random.uniform(-1, 1, (2 * num_test, latent_size))\n",
    "        sampled_labels = np.random.randint(0, num_classes, 2 * num_test)\n",
    "\n",
    "        trick = np.ones(2 * num_test)\n",
    "\n",
    "        generator_test_loss = combined.evaluate(\n",
    "            [noise, sampled_labels.reshape((-1, 1))],\n",
    "            [trick, sampled_labels], verbose=False)\n",
    "\n",
    "        generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n",
    "\n",
    "        train_history['generator'].append(generator_train_loss)\n",
    "        train_history['discriminator'].append(discriminator_train_loss)\n",
    "\n",
    "        test_history['generator'].append(generator_test_loss)\n",
    "        test_history['discriminator'].append(discriminator_test_loss)\n",
    "\n",
    "        print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
    "            'component', *discriminator.metrics_names))\n",
    "        print('-' * 65)\n",
    "\n",
    "        ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.4f} | {3:<5.4f}'\n",
    "        print(ROW_FMT.format('generator (train)',\n",
    "                             *train_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('generator (test)',\n",
    "                             *test_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (train)',\n",
    "                             *train_history['discriminator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (test)',\n",
    "                             *test_history['discriminator'][-1]))\n",
    "        generator.save_weights(\n",
    "            'params_generator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "        discriminator.save_weights(\n",
    "            'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "\n",
    "\n",
    "        num_rows = 40\n",
    "        noise = np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)),\n",
    "                        (num_classes, 1))\n",
    "\n",
    "        sampled_labels = np.array([\n",
    "            [i] * num_rows for i in range(num_classes)\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "        generated_images = generator.predict(\n",
    "            [noise, sampled_labels], verbose=0)\n",
    "\n",
    "        real_labels = y_train[(epoch - 1) * num_rows * num_classes:\n",
    "                              epoch * num_rows * num_classes]\n",
    "        indices = np.argsort(real_labels, axis=0)\n",
    "        real_images = x_train[(epoch - 1) * num_rows * num_classes:\n",
    "                              epoch * num_rows * num_classes][indices]\n",
    "\n",
    "        img = np.concatenate(\n",
    "            (generated_images,\n",
    "             np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0),\n",
    "             real_images))\n",
    "\n",
    "        img = (np.concatenate([r.reshape(-1, 28)\n",
    "                               for r in np.split(img, 2 * num_classes + 1)\n",
    "                               ], axis=-1) * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "        Image.fromarray(img).save(\n",
    "            'plot_epoch_{0:03d}_generated.png'.format(epoch))\n",
    "\n",
    "    with open('acgan-history.pkl', 'wb') as f:\n",
    "        pickle.dump({'train': train_history, 'test': test_history}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
